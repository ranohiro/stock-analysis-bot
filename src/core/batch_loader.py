import os
import requests
import pandas as pd
import sqlite3
from datetime import datetime, timedelta
from dotenv import load_dotenv
import io
import time
import jpholiday
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from src.core.db_manager import get_connection
from typing import Union

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()
KABU_PLUS_USER = os.getenv('KABU_PLUS_USER')
KABU_PLUS_PASSWORD = os.getenv('KABU_PLUS_PASSWORD')

# æ ªãƒ»ãƒ—ãƒ©ã‚¹ã®ãƒ™ãƒ¼ã‚¹URL
KABU_PLUS_BASE_URL = 'https://csvex.com/kabu.plus/csv/'
TIMEOUT = 30
ENCODING = 'cp932'

# --- æ¥ç¶šè¨­å®š ---
def make_session_with_retries():
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®requestsã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    s = requests.Session()
    retries = Retry(total=3, backoff_factor=0.5,
                    status_forcelist=[429, 500, 502, 503, 504],
                    allowed_methods=["GET"])
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.headers.update({
        "User-Agent": "StockAnalysisBot/1.0"
    })
    return s

def fetch_csv_as_dataframe(url: str, session: requests.Session, skiprows: int = 0):
    """URLã‹ã‚‰CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€Pandas DataFrameã¨ã—ã¦è¿”ã™"""
    auth_tuple = (KABU_PLUS_USER, KABU_PLUS_PASSWORD)
    
    try:
        response = session.get(url, auth=auth_tuple, timeout=TIMEOUT)
        response.raise_for_status()
        df = pd.read_csv(io.BytesIO(response.content), encoding=ENCODING, skiprows=skiprows)
        return df

    except requests.exceptions.HTTPError as e:
        if response.status_code == 404:
            print(f"  -> ã‚¹ã‚­ãƒƒãƒ—: {url.split('/')[-1]} (404 Not Found)")
        elif response.status_code == 401:
            print(f"  -> ã‚¨ãƒ©ãƒ¼: 401 Unauthorized")
        else:
            print(f"  -> ã‚¨ãƒ©ãƒ¼: HTTP {e}")
    except Exception as e:
        print(f"  -> ã‚¨ãƒ©ãƒ¼: {e}")
    return None


# --- 1. æ—¥è¶³æ ªä¾¡ & ä¼æ¥­ãƒã‚¹ã‚¿æ›´æ–° ---
def insert_daily_prices(date_str: str, conn: sqlite3.Connection, session: requests.Session):
    filename = f"japan-all-stock-prices-2_{date_str}.csv"
    url = f"{KABU_PLUS_BASE_URL}japan-all-stock-prices-2/daily/{filename}"
    
    df = fetch_csv_as_dataframe(url, session, skiprows=0)
    if df is None: return

    try:
        # DBã«æ ¼ç´ã™ã‚‹ã‚«ãƒ©ãƒ ï¼ˆDBåã¯camel_caseï¼‰ã¨CSVãƒ˜ãƒƒãƒ€ãƒ¼åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        col_map = {
            'SC': 'code',
            'åç§°': 'name',       # ä¼æ¥­åã‚’ä¿å­˜
            'å¸‚å ´': 'market',     # å¸‚å ´åŒºåˆ†
            'æ¥­ç¨®': 'industry',   # æ¥­ç¨®
            'æ—¥ä»˜': 'date',
            'å§‹å€¤': 'open',
            'é«˜å€¤': 'high',
            'å®‰å€¤': 'low',
            'æ ªä¾¡': 'close',
            'å‡ºæ¥é«˜': 'volume',
            'å£²è²·ä»£é‡‘ï¼ˆåƒå††ï¼‰': 'trading_value',
            'æ™‚ä¾¡ç·é¡ï¼ˆç™¾ä¸‡å††ï¼‰': 'market_cap_total'
        }
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ (å¿…é ˆã‚«ãƒ©ãƒ ã®ã¿)
        must_have = ['SC', 'åç§°', 'æ—¥ä»˜', 'æ ªä¾¡']
        missing = [c for c in must_have if c not in df.columns]
        if missing:
            print(f"  -> ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šç¾åœ¨ã®DFã‚«ãƒ©ãƒ : {list(df.columns)}")
            raise KeyError(f"CSVã«å¿…é ˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}")

        # ãƒªãƒãƒ¼ãƒ å¯¾è±¡ã®CSVã‚«ãƒ©ãƒ ã‚’æŠ½å‡ºï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰
        valid_cols = {csv_name: db_name for csv_name, db_name in col_map.items() if csv_name in df.columns}
        df.rename(columns=valid_cols, inplace=True)
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        df['date'] = date_str 
        df['code'] = df['code'].astype(str)

        # --- A. ä¼æ¥­ãƒã‚¹ã‚¿ (companies) ã®æ›´æ–° ---
        # æ¯æ—¥æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€ç¤¾åå¤‰æ›´ã‚„æ–°è¦ä¸Šå ´ã«å¯¾å¿œ
        companies_df = df[['code', 'name', 'market', 'industry']].copy()
        companies_df.drop_duplicates(subset=['code'], inplace=True)
        
        comp_records = [tuple(x) for x in companies_df.where(pd.notnull(companies_df), None).to_numpy()]
        conn.executemany("""
            INSERT OR REPLACE INTO companies (code, name, market, industry)
            VALUES (?, ?, ?, ?)
        """, comp_records)

        # --- B. æ—¥è¶³æ ªä¾¡ (daily_prices) ã®æ›´æ–° ---
        # æ—¢å­˜ã‚«ãƒ©ãƒ ã«åŠ ãˆã€å£²è²·ä»£é‡‘ã¨æ™‚ä¾¡ç·é¡ï¼ˆå…¨éŠ˜æŸ„ï¼‰ã‚’è¿½åŠ 
        prices_db_cols = ['code', 'date', 'open', 'high', 'low', 'close', 'volume', 'trading_value', 'market_cap_total']
        prices_df = df[[c for c in prices_db_cols if c in df.columns]].copy()
        
        # DBã‚«ãƒ©ãƒ ã«ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯Noneã‚’è¿½åŠ 
        for col in prices_db_cols:
            if col not in prices_df.columns:
                prices_df[col] = None

        price_records = [tuple(row) for row in prices_df.where(pd.notnull(prices_df), None).itertuples(index=False)]

        conn.executemany(f"""
            INSERT OR REPLACE INTO daily_prices ({', '.join(prices_db_cols)}) 
            VALUES ({', '.join(['?'] * len(prices_db_cols))})
        """, price_records)
        
        print(f"  -> æ ªä¾¡ãƒ»ä¼æ¥­æƒ…å ±: {len(price_records)}ä»¶ å‡¦ç†å®Œäº†")

    except Exception as e:
        print(f"  -> ã‚¨ãƒ©ãƒ¼(æ ªä¾¡): {e}")


# --- 2. è²¡å‹™æŒ‡æ¨™ ---
def insert_daily_financials(date_str: str, conn: sqlite3.Connection, session: requests.Session):
    filename = f"japan-all-stock-data_{date_str}.csv"
    url = f"{KABU_PLUS_BASE_URL}japan-all-stock-data/daily/{filename}"
    
    # è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚‚1è¡Œç›®ãŒãƒ˜ãƒƒãƒ€ãƒ¼ãªã®ã§ skiprows=0
    df = fetch_csv_as_dataframe(url, session, skiprows=0)
    if df is None: return
    
    try:
        col_map = {
            'SC': 'code',
            'æ™‚ä¾¡ç·é¡ï¼ˆç™¾ä¸‡å††ï¼‰': 'market_cap',
            'ç™ºè¡Œæ¸ˆæ ªå¼æ•°': 'shares_outstanding',
            'é…å½“åˆ©å›ã‚Šï¼ˆäºˆæƒ³ï¼‰': 'dividend_yield',
            'PERï¼ˆäºˆæƒ³ï¼‰': 'per_forecast',
            'PBRï¼ˆå®Ÿç¸¾ï¼‰': 'pbr_actual',
            'EPSï¼ˆäºˆæƒ³ï¼‰': 'eps_forecast',
            'BPSï¼ˆå®Ÿç¸¾ï¼‰': 'bps_actual',
            'æœ€ä½æŠ•è³‡é‡‘é¡': 'min_investment'
        }
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ (å¿…é ˆã‚«ãƒ©ãƒ ã®ã¿)
        must_have = ['SC', 'PERï¼ˆäºˆæƒ³ï¼‰', 'PBRï¼ˆå®Ÿç¸¾ï¼‰']
        missing = [c for c in must_have if c not in df.columns]
        if missing:
            # æ™‚ä¾¡ç·é¡ã®ã‚«ãƒ©ãƒ åãŒæºã‚Œã‚‹å¯èƒ½æ€§ã«å¯¾å¿œ
            if 'æ™‚ä¾¡ç·é¡ï¼ˆå…¨éŠ˜æŸ„ï¼‰' in df.columns:
                df.rename(columns={'æ™‚ä¾¡ç·é¡ï¼ˆå…¨éŠ˜æŸ„ï¼‰': 'market_cap'}, inplace=True)
                col_map.pop('æ™‚ä¾¡ç·é¡ï¼ˆç™¾ä¸‡å††ï¼‰', None) 
            else:
                raise KeyError(f"CSVã«å¿…é ˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}")
        
        # ãƒªãƒãƒ¼ãƒ 
        valid_cols = {csv_name: db_name for csv_name, db_name in col_map.items() if csv_name in df.columns}
        df.rename(columns=valid_cols, inplace=True)
        
        # DBæŒ¿å…¥ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆ
        fin_db_cols = ['code', 'date', 'market_cap', 'shares_outstanding', 'per_forecast', 'pbr_actual', 
                    'eps_forecast', 'bps_actual', 'dividend_yield', 'min_investment']

        df['date'] = date_str
        df['code'] = df['code'].astype(str)
        
        # DBã‚«ãƒ©ãƒ ã«ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯Noneã‚’è¿½åŠ 
        for col in fin_db_cols:
            if col not in df.columns:
                df[col] = None
        
        # DBæŒ¿å…¥é †ã«ä¸¦ã¹æ›¿ãˆ
        df = df[fin_db_cols]

        records = [tuple(row) for row in df.where(pd.notnull(df), None).itertuples(index=False)]

        conn.executemany(f"""
            INSERT OR REPLACE INTO daily_financials ({', '.join(fin_db_cols)}) 
            VALUES ({', '.join(['?'] * len(fin_db_cols))})
        """, records)
        print(f"  -> è²¡å‹™æŒ‡æ¨™: {len(records)}ä»¶ å‡¦ç†å®Œäº†")
        
    except Exception as e:
        print(f"  -> ã‚¨ãƒ©ãƒ¼(è²¡å‹™): {e}")


# --- 3. ä¿¡ç”¨æ®‹ ---
def insert_weekly_margin(date_str: str, conn: sqlite3.Connection, session: requests.Session):
    # ç¥æ—¥ãƒã‚§ãƒƒã‚¯ï¼šé€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒå…¬è¡¨ã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å¸‚å ´å–¶æ¥­æ—¥ã®ã¿å‡¦ç†
    download_date = datetime.strptime(date_str, '%Y%m%d').date()
    if jpholiday.is_holiday(download_date): # ç¥æ—¥ãƒã‚§ãƒƒã‚¯
        print(f"  -> ã‚¹ã‚­ãƒƒãƒ—: {date_str} (å¸‚å ´ä¼‘æ¥­æ—¥/ç¥æ—¥)")
        return
    
    filename = f"tosho-stock-margin-transactions-2_{date_str}.csv"
    url = f"{KABU_PLUS_BASE_URL}tosho-stock-margin-transactions-2/weekly/{filename}"
    
    df = fetch_csv_as_dataframe(url, session, skiprows=0)
    if df is None: return

    try:
        original_cols = ["SC","å…¬è¡¨æ—¥","ä¿¡ç”¨å–å¼•åŒºåˆ†","ä¿¡ç”¨å£²æ®‹","ä¿¡ç”¨å£²æ®‹ å‰é€±æ¯”","ä¿¡ç”¨è²·æ®‹","ä¿¡ç”¨è²·æ®‹ å‰é€±æ¯”","è²¸å€Ÿå€ç‡", "åˆ¶åº¦ä¿¡ç”¨å£²æ®‹", "åˆ¶åº¦ä¿¡ç”¨å£²æ®‹ å‰é€±æ¯”", "åˆ¶åº¦ä¿¡ç”¨è²·æ®‹", "åˆ¶åº¦ä¿¡ç”¨è²·æ®‹ å‰é€±æ¯”", "ä¸€èˆ¬ä¿¡ç”¨å£²æ®‹", "ä¸€èˆ¬ä¿¡ç”¨å£²æ®‹ å‰é€±æ¯”", "ä¸€èˆ¬ä¿¡ç”¨è²·æ®‹", "ä¸€èˆ¬ä¿¡ç”¨è²·æ®‹ å‰é€±æ¯”"]
        
        if len(df.columns) == len(original_cols):
            df.columns = original_cols
        else:
            raise KeyError(f"ã‚«ãƒ©ãƒ æ•°ä¸ä¸€è‡´: CSV({len(df.columns)}) vs æœŸå¾…å€¤({len(original_cols)})")

        # --- æ—¥ä»˜è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç¥æ—¥å¯¾å¿œç‰ˆï¼‰---
        # 1. å…¬è¡¨æ—¥ï¼ˆé€šå¸¸ç«æ›œãªã©ï¼‰ã‹ã‚‰ã€ãƒ‡ãƒ¼ã‚¿ãŒæŒ‡ã—ç¤ºã™ã€Œå‰é€±ã®é‡‘æ›œæ—¥ã€ã‚’è¨ˆç®—
        current_date = datetime.strptime(date_str, '%Y%m%d').date()
        
        # åœŸæ›œãƒ»æ—¥æ›œãƒ»å…¬è¡¨æ—¥å½“æ—¥ã‚’èµ·ç‚¹ã«ã—ãªã„ã‚ˆã†ã«ã€ã¾ãšæœˆæ›œã¾ã§ç§»å‹•
        if current_date.weekday() >= 5: # åœŸæ—¥ãªã‚‰
            days_to_subtract = current_date.weekday() - 4
        else:
            days_to_subtract = current_date.weekday() + 1
            
        data_date = current_date - timedelta(days=days_to_subtract)
        
        # 2. å¸‚å ´å–¶æ¥­æ—¥ã¾ã§ã•ã‹ã®ã¼ã‚‹ï¼ˆç¥æ—¥ãƒ»é‡‘æ›œæ—¥ãŒä¼‘å ´æ—¥å¯¾å¿œï¼‰
        while data_date.weekday() >= 5 or jpholiday.is_holiday(data_date):
            data_date -= timedelta(days=1)
            
        found_date_str = data_date.strftime('%Y%m%d')
        df['date'] = found_date_str 

        # æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤ (æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒãªã„è¡Œã‚’é™¤ããŸã‚)
        df.dropna(subset=['ä¿¡ç”¨å£²æ®‹', 'ä¿¡ç”¨è²·æ®‹'], inplace=True) 
        
        # DBã«æ ¼ç´ã™ã‚‹ã‚«ãƒ©ãƒ ã¨CSVãƒ˜ãƒƒãƒ€ãƒ¼åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        col_map = {
            'SC': 'code',
            'ä¿¡ç”¨å£²æ®‹': 'sell_balance_total', 'ä¿¡ç”¨è²·æ®‹': 'buy_balance_total',
            'è²¸å€Ÿå€ç‡': 'ratio', 'åˆ¶åº¦ä¿¡ç”¨å£²æ®‹': 'sell_balance_ins', 
            'åˆ¶åº¦ä¿¡ç”¨è²·æ®‹': 'buy_balance_ins', 'ä¸€èˆ¬ä¿¡ç”¨å£²æ®‹': 'sell_balance_gen', 
            'ä¸€èˆ¬ä¿¡ç”¨è²·æ®‹': 'buy_balance_gen'
        }
        
        valid_cols = {csv_name: db_name for csv_name, db_name in col_map.items() if csv_name in df.columns}
        df.rename(columns=valid_cols, inplace=True)
        
        margin_db_cols = ['code', 'date', 'sell_balance_total', 'buy_balance_total', 'ratio', 
                        'sell_balance_ins', 'buy_balance_ins', 'sell_balance_gen', 'buy_balance_gen']
        
        df['code'] = df['code'].astype(str)
        
        for col in margin_db_cols:
            if col not in df.columns:
                df[col] = None
        
        df = df[margin_db_cols]

        records = [tuple(row) for row in df.where(pd.notnull(df), None).itertuples(index=False)]

        conn.executemany(f"""
            INSERT OR REPLACE INTO weekly_margin ({', '.join(margin_db_cols)}) 
            VALUES ({', '.join(['?'] * len(margin_db_cols))})
        """, records)
        print(f"  -> ä¿¡ç”¨æ®‹: {len(records)}ä»¶ å‡¦ç†å®Œäº† (ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜: {found_date_str})")
        
    except Exception as e:
        print(f"  -> ã‚¨ãƒ©ãƒ¼(ä¿¡ç”¨æ®‹): {e}")
        print(f"  -> ãƒ‡ãƒãƒƒã‚°æƒ…å ±(ä¿¡ç”¨æ®‹): DFã‚«ãƒ©ãƒ : {list(df.columns)}") # ã‚ªãƒ—ã‚·ãƒ§ãƒ³


# --- 4. æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ (æ±è¨¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥æŒ‡æ•°) ---
def insert_daily_indices(date_str: str, conn: sqlite3.Connection, session: requests.Session):
    filename = f"tosho-index-data_{date_str}.csv"
    url = f"{KABU_PLUS_BASE_URL}tosho-index-data/daily/{filename}"

    df = fetch_csv_as_dataframe(url, session, skiprows=0)
    if df is None: return

    try:
        # ğŸŒŸ ã”æç¤ºã„ãŸã ã„ãŸæ­£ã—ã„æ—¥æœ¬èªãƒ˜ãƒƒãƒ€ãƒ¼åã‚’ä½¿ç”¨
        original_cols = ["SC","æŒ‡æ•°å","æ—¥ä»˜","çµ‚å€¤","å‰æ—¥æ¯”","å‰æ—¥æ¯”ï¼ˆï¼…ï¼‰","å‰æ—¥çµ‚å€¤","æ™‚ä¾¡ç·é¡ï¼ˆæŒ‡æ•°ç”¨ãƒ»æµ®å‹•æ ªãƒ™ãƒ¼ã‚¹ï¼‰","æ™‚ä¾¡ç·é¡å‰æ—¥æ¯”ï¼ˆåŒå·¦ï¼‰","å‰æ—¥æ™‚ä¾¡ç·é¡ï¼ˆåŒå·¦ï¼‰","å¹³å‡æ™‚ä¾¡ç·é¡ï¼ˆåŒå·¦ï¼‰","åŸºæº–æ™‚ä¾¡ç·é¡","éŠ˜æŸ„æ•°","å£²è²·å˜ä½æ›ç®—å¾Œæ ªå¼æ•°"]

        # CSVã®ãƒ˜ãƒƒãƒ€ãƒ¼æ•°ãŒåˆè‡´ã™ã‚‹ã‹ç¢ºèª
        if len(df.columns) == len(original_cols):
            df.columns = original_cols
        else:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã®æ‹¬å¼§ãŒãªã„å¯èƒ½æ€§ã‚‚è€ƒæ…®ã—ãŸãƒã‚§ãƒƒã‚¯
            alt_original_cols = ["SC","æŒ‡æ•°å","æ—¥ä»˜","çµ‚å€¤","å‰æ—¥æ¯”","å‰æ—¥æ¯”ï¼ˆï¼…ï¼‰","å‰æ—¥çµ‚å€¤","æ™‚ä¾¡ç·é¡ï¼ˆæŒ‡æ•°ç”¨ãƒ»æµ®å‹•æ ªãƒ™ãƒ¼ã‚¹ï¼‰","æ™‚ä¾¡ç·é¡å‰æ—¥æ¯”","å‰æ—¥æ™‚ä¾¡ç·é¡","å¹³å‡æ™‚ä¾¡ç·é¡","åŸºæº–æ™‚ä¾¡ç·é¡","éŠ˜æŸ„æ•°","å£²è²·å˜ä½æ›ç®—å¾Œæ ªå¼æ•°"]
            if len(df.columns) == len(alt_original_cols):
                df.columns = alt_original_cols
            else:
                raise KeyError(f"ã‚«ãƒ©ãƒ æ•°ä¸ä¸€è‡´: CSV({len(df.columns)}) vs æœŸå¾…å€¤({len(original_cols)})")

        # DBã«æ ¼ç´ã™ã‚‹ã‚«ãƒ©ãƒ ã¨CSVãƒ˜ãƒƒãƒ€ãƒ¼åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        col_map = {
            'SC': 'code',
            'æŒ‡æ•°å': 'name',
            'æ—¥ä»˜': 'date',
            'çµ‚å€¤': 'close',
            'å‰æ—¥æ¯”ï¼ˆï¼…ï¼‰': 'change_ratio',
            'æ™‚ä¾¡ç·é¡ï¼ˆæŒ‡æ•°ç”¨ãƒ»æµ®å‹•æ ªãƒ™ãƒ¼ã‚¹ï¼‰': 'market_cap_index',
            'å£²è²·å˜ä½æ›ç®—å¾Œæ ªå¼æ•°': 'volume', # DBã‚«ãƒ©ãƒ : volume
            'éŠ˜æŸ„æ•°': 'éŠ˜æŸ„æ•°' # DBã‚«ãƒ©ãƒ : éŠ˜æŸ„æ•°
        }

        # å®Ÿéš›ã®DataFrameã®ã‚«ãƒ©ãƒ åã¨DBåã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
        valid_cols = {}
        for csv_name in df.columns:
            if csv_name in col_map:
                valid_cols[csv_name] = col_map[csv_name]
        
        df.rename(columns=valid_cols, inplace=True)
        
        # æœ€çµ‚çš„ãªDBæ ¼ç´ã‚«ãƒ©ãƒ  (db_managerã§å®šç¾©ã—ãŸã‚«ãƒ©ãƒ å)
        index_db_cols = ['code', 'name', 'date', 'close', 'change_ratio', 'market_cap_index', 'volume', 'éŠ˜æŸ„æ•°']
        
        df['date'] = date_str
        df['code'] = df['code'].astype(str)

        # æ¬ æå€¤å¯¾å¿œ
        for col in index_db_cols:
            if col not in df.columns:
                df[col] = None
        
        # DBæŒ¿å…¥é †ã«ä¸¦ã¹æ›¿ãˆ
        df = df[index_db_cols]

        records = [tuple(row) for row in df.where(pd.notnull(df), None).itertuples(index=False)]

        conn.executemany(f"""
            INSERT OR REPLACE INTO daily_indices ({', '.join(index_db_cols)}) 
            VALUES ({', '.join(['?'] * len(index_db_cols))})
        """, records)
        print(f"  -> æ¥­ç¨®åˆ¥æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿: {len(records)}ä»¶ å‡¦ç†å®Œäº†")
        
    except Exception as e:
        print(f"  -> ã‚¨ãƒ©ãƒ¼(æ¥­ç¨®åˆ¥æŒ‡æ•°ãƒ‡ãƒ¼ã‚¿): {e}")

def run_daily_batch(start_date_str: str, end_date_str: str):
    """
    æŒ‡å®šæœŸé–“ã®æ—¥æ¬¡/é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´ã™ã‚‹ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
    """
    if not all([KABU_PLUS_USER, KABU_PLUS_PASSWORD]):
        print("âŒ ã‚¨ãƒ©ãƒ¼: .envãƒ•ã‚¡ã‚¤ãƒ«ã«KABU_PLUS_USERã¾ãŸã¯KABU_PLUS_PASSWORDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    session = make_session_with_retries()
    start_date = datetime.strptime(start_date_str, '%Y%m%d')
    end_date = datetime.strptime(end_date_str, '%Y%m%d')
    
    print(f"=== ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {start_date_str} ~ {end_date_str} ===")
    
    dates = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]
    
    with get_connection() as conn:
        for date in dates:
            date_str = date.strftime('%Y%m%d')
            
            # åœŸæ—¥ã¯ã‚¹ã‚­ãƒƒãƒ—
            if date.weekday() >= 5: continue
            
            # ç¥æ—¥ã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç„¡é§„ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’é˜²ãï¼‰
            if jpholiday.is_holiday(date):
                print(f"Skipping: {date_str} (Holiday)")
                continue
                
            print(f"Processing: {date_str}")
            insert_daily_prices(date_str, conn, session)
            insert_daily_financials(date_str, conn, session)
            insert_weekly_margin(date_str, conn, session)
            insert_daily_indices(date_str, conn, session)
            
            time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        
        conn.commit()
        print("\n=== âœ… å…¨å‡¦ç†å®Œäº† ===")

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Stock Data Batch Loader')
    parser.add_argument('--days', type=int, default=0, help='Past days to fetch (default: 0 = Today only)')
    args = parser.parse_args()

    # æŒ‡å®šæ—¥æ•°åˆ†ã‚’å–å¾—
    end_date = datetime.now()
    start_date = end_date - timedelta(days=args.days)
    
    run_daily_batch(start_date.strftime('%Y%m%d'), end_date.strftime('%Y%m%d'))
